<p align="center">
    <br>
    <img src="imgs/ç²µ.png" width="100"/>
    <br>
</p>

<h3 align="center">
<p><b>Cantoformer <br/>å»£æ±è©±å˜…èªè¨€ AI</b> </p>
</h3>

Recent advances in AI enable smarter applications based on texts. 
It's good but they are mostly in English due to its abundance of texts available from the Internet.

This repository explores LM in **Cantonese (Yue Chinese, å»£æ±è©±)**, a langauge predominantly spoken in Guangzhou, Hong Kong and Macau, and containing very challenging lingual properties for AI to learn.

æœ€è¿‘å˜… AI è¬›åˆ°å¥½å·´é–‰å’ï¼Œåˆè©±å’©å’© chatbot å¯ä»¥å¥½ä¼¼çœŸäººå’åŒäººå°ç­”ï¼Œä½†å…¶å¯¦å–ºå‘¢å€‹ã€Œèªè¨€è™•ç†ã€å˜…é ˜åŸŸå…¥é¢ï¼Œå¥½å¤šå˜…è³‡æºéƒ½åªä¿‚å¾—è‹±æ–‡ï¼Œæ‰€ä»¥è¦è½æ‰‹åšå»£æ±è©± NLPï¼Œå…¶å¯¦å””å®¹æ˜“ã€‚

æ‰€ä»¥è«—ä½å–ºå‘¢åº¦é–‹å€‹ Repo ï¼Œé¼“å‹µæ›´å¤šäººé–‹ç™¼å»£æ±è©± AIã€‚

## Challenges

- Mixed Languages (English, Chinese, Yue)
- Complex Syntax
- Scarce Resource
- Many Homonyms & Homophones

## Framework to be used

- `Pytorch`
- `ğŸ¤—Transformers`

## Libraries to be used

- `ğŸ¤—Tokenizers`
- `Jieba`

## Data to be used

- English
  - [Wiki](https://dumps.wikimedia.org/)
  - [BookCorpus](https://github.com/soskek/bookcorpus)
  - News Articles
  - Forum Data

- Chinese/Cantonese
  - [Wiki](https://dumps.wikimedia.org/)
  - News Articles
  - Forum Data


## To Do List

- [x] Apply normalization on Chinese characters
- [ ] ELECTRA-base + Whole Word Masking Preprocessing **(Working)**
- [ ] ELECTRA-base + Whole Word Masking Pretraining
- [ ] Finetuning on EN-MNLI
- [ ] Compare results to existing benchmarks
- [ ] Release `canto-mnli` dataset for evaluation
      


## Benchmarks

### **ELECTRA-base / EN-MNLI**
|   ( Acc. )    |      Dev  |
| ------------- |:------------:|
| Ours          |     ?    |
| [Google](https://github.com/google-research/electra)  | 88.8 |


### **ELECTRA-base / DRCD**
|   ( EM / F1 )    |      Dev      |  Test  |
| ------------- |:-------------:|:------:|
| Ours          |        ?      |     ?  |
| [HFL](https://github.com/ymcui/Chinese-ELECTRA)  | 87.5 / 92.5        |   86.9 / 91.8 |

### **ELECTRA-small / EN-MNLI**
|   ( Acc. )    |      Dev  |
| ------------- |:------------:|
| Ours (1M steps)          |     75.9    |
| Ours (4M steps)          |     77.1    |
| Google (1M steps)          |     79.7    |
| Google (4M steps)          |     81.6    |

## References

### Expected Losses / Training Curves during Pre-Training.

https://github.com/google-research/electra/issues/3
