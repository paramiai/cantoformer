
        
from tokenizers import Tokenizer, AddedToken, pre_tokenizers, decoders, trainers
from tokenizers.models import WordPiece
from tokenizers.normalizers import NFKC, Lowercase, Sequence

from tokenizers.normalizers import BertNormalizer
from tokenizers.pre_tokenizers import BertPreTokenizer
from tokenizers.processors import BertProcessing
from typing import Optional, List, Union

from tokenizers.models import BPE
from tokenizers.implementations.base_tokenizer import BaseTokenizer

SPECIAL_CHARS = (
    
    # Emojis
    
    '¬©¬Æ‚Äº‚Åâ‚É£‚Ñ¢‚Ñπ‚Üî‚Üï‚Üñ‚Üó‚Üò‚Üô‚Ü©‚Ü™‚åö‚åõ‚å®‚èè‚è©‚è™‚è´‚è¨‚è≠‚èÆ‚èØ‚è∞‚è±‚è≤‚è≥‚è∏‚èπ‚è∫‚ìÇ‚ñ™‚ñ´‚ñ∂‚óÄ‚óª‚óº‚óΩ‚óæ‚òÄ‚òÅ‚òÇ‚òÉ‚òÑ‚òé‚òë‚òî‚òï‚òò‚òù‚ò†‚ò¢‚ò£‚ò¶‚ò™‚òÆ‚òØ‚ò∏‚òπ‚ò∫‚ôÄ‚ôÇ‚ôà‚ôâ‚ôä‚ôã‚ôå‚ôç‚ôé‚ôè‚ôê‚ôë‚ôí‚ôì‚ôü‚ô†‚ô£‚ô•‚ô¶‚ô®‚ôª‚ôæ‚ôø‚öí‚öì‚öî‚öï‚öñ‚öó‚öô‚öõ‚öú‚ö†‚ö°‚öß‚ö™‚ö´‚ö∞‚ö±‚öΩ‚öæ‚õÑ‚õÖ‚õà‚õé‚õè‚õë‚õì‚õî‚õ©‚õ™‚õ∞‚õ±‚õ≤‚õ≥‚õ¥‚õµ‚õ∑‚õ∏‚õπ‚õ∫‚õΩ‚úÇ‚úÖ‚úà‚úâ‚úä‚úã‚úå‚úç‚úè‚úí‚úî‚úñ‚úù‚ú°‚ú®‚ú≥‚ú¥‚ùÑ‚ùá‚ùå‚ùé‚ùì‚ùî‚ùï‚ùó‚ù£‚ù§‚ûï‚ûñ‚ûó‚û°‚û∞‚ûø‚§¥‚§µ‚¨Ö‚¨Ü‚¨á‚¨õ‚¨ú‚≠ê‚≠ï„Ä∞„ÄΩ„äó„äôÔ∏èüÄÑüÉèüÖ∞üÖ±üÖæüÖøüÜéüÜëüÜíüÜìüÜîüÜïüÜñüÜóüÜòüÜôüÜöüá¶üáßüá®üá©üá™üá´üá¨üá≠üáÆüáØüá∞üá±üá≤üá≥üá¥üáµüá∂üá∑üá∏üáπüá∫üáªüáºüáΩüáæüáøüàÅüàÇüàöüàØüà≤üà≥üà¥üàµüà∂üà∑üà∏üàπüà∫üâêüâëüåÄüåÅüåÇüåÉüåÑüåÖüåÜüåáüåàüåâüåäüåãüååüåçüåéüåèüåêüåëüåíüåìüåîüåïüåñüåóüåòüåôüåöüåõüåúüåùüåûüåüüå†üå°üå§üå•üå¶üåßüå®üå©üå™üå´üå¨üå≠üåÆüåØüå∞üå±üå≤üå≥üå¥üåµüå∂üå∑üå∏üåπüå∫üåªüåºüåΩüåæüåøüçÄüçÅüçÇüçÉüçÑüçÖüçÜüçáüçàüçâüçäüçãüçåüççüçéüçèüçêüçëüçíüçìüçîüçïüçñüçóüçòüçôüçöüçõüçúüçùüçûüçüüç†üç°üç¢üç£üç§üç•üç¶üçßüç®üç©üç™üç´üç¨üç≠üçÆüçØüç∞üç±üç≤üç≥üç¥üçµüç∂üç∑üç∏üçπüç∫üçªüçºüçΩüçæüçøüéÄüéÅüéÇüéÉüéÑüéÖüéÜüéáüéàüéâüéäüéãüéåüéçüééüéèüéêüéëüéíüéìüéñüéóüéôüéöüéõüéûüéüüé†üé°üé¢üé£üé§üé•üé¶üéßüé®üé©üé™üé´üé¨üé≠üéÆüéØüé∞üé±üé≤üé≥üé¥üéµüé∂üé∑üé∏üéπüé∫üéªüéºüéΩüéæüéøüèÄüèÅüèÇüèÉüèÑüèÖüèÜüèáüèàüèâüèäüèãüèåüèçüèéüèèüèêüèëüèíüèìüèîüèïüèñüèóüèòüèôüèöüèõüèúüèùüèûüèüüè†üè°üè¢üè£üè§üè•üè¶üèßüè®üè©üè™üè´üè¨üè≠üèÆüèØüè∞üè≥üè¥üèµüè∑üè∏üèπüè∫üêÄüêÅüêÇüêÉüêÑüêÖüêÜüêáüêàüêâüêäüêãüêåüêçüêéüêèüêêüêëüêíüêìüêîüêïüêñüêóüêòüêôüêöüêõüêúüêùüêûüêüüê†üê°üê¢üê£üê§üê•üê¶üêßüê®üê©üê™üê´üê¨üê≠üêÆüêØüê∞üê±üê≤üê≥üê¥üêµüê∂üê∑üê∏üêπüê∫üêªüêºüêΩüêæüêøüëÄüëÅüëÇüëÉüëÑüëÖüëÜüëáüëàüëâüëäüëãüëåüëçüëéüëèüëêüëëüëíüëìüëîüëïüëñüëóüëòüëôüëöüëõüëúüëùüëûüëüüë†üë°üë¢üë£üë§üë•üë¶üëßüë®üë©üë™üë´üë¨üë≠üëÆüëØüë∞üë±üë≤üë≥üë¥üëµüë∂üë∑üë∏üëπüë∫üëªüëºüëΩüëæüëøüíÄüíÅüíÇüíÉüíÑüíÖüíÜüíáüíàüíâüíäüíãüíåüíçüíéüíèüíêüíëüííüíìüíîüíïüíñüíóüíòüíôüíöüíõüíúüíùüíûüíüüí†üí°üí¢üí£üí§üí•üí¶üíßüí®üí©üí™üí´üí¨üí≠üíÆüíØüí∞üí±üí≤üí≥üí¥üíµüí∂üí∑üí∏üíπüí∫üíªüíºüíΩüíæüíøüìÄüìÅüìÇüìÉüìÑüìÖüìÜüìáüìàüìâüìäüìãüìåüìçüìéüìèüìêüìëüìíüììüìîüìïüìñüìóüìòüìôüìöüìõüìúüìùüìûüìüüì†üì°üì¢üì£üì§üì•üì¶üìßüì®üì©üì™üì´üì¨üì≠üìÆüìØüì∞üì±üì≤üì≥üì¥üìµüì∂üì∑üì∏üìπüì∫üìªüìºüìΩüìøüîÄüîÅüîÇüîÉüîÑüîÖüîÜüîáüîàüîâüîäüîãüîåüîçüîéüîèüîêüîëüîíüîìüîîüîïüîñüîóüîòüîôüîöüîõüîúüîùüîûüîüüî†üî°üî¢üî£üî§üî•üî¶üîßüî®üî©üî™üî´üî¨üî≠üîÆüîØüî∞üî±üî≤üî≥üî¥üîµüî∂üî∑üî∏üîπüî∫üîªüîºüîΩüïâüïäüïãüïåüïçüïéüïêüïëüïíüïìüïîüïïüïñüïóüïòüïôüïöüïõüïúüïùüïûüïüüï†üï°üï¢üï£üï§üï•üï¶üïßüïØüï∞üï≥üï¥üïµüï∂üï∑üï∏üïπüï∫üñáüñäüñãüñåüñçüñêüñïüññüñ§üñ•üñ®üñ±üñ≤üñºüóÇüóÉüóÑüóëüóíüóìüóúüóùüóûüó°üó£üó®üóØüó≥üó∫üóªüóºüóΩüóæüóøüòÄüòÅüòÇüòÉüòÑüòÖüòÜüòáüòàüòâüòäüòãüòåüòçüòéüòèüòêüòëüòíüòìüòîüòïüòñüòóüòòüòôüòöüòõüòúüòùüòûüòüüò†üò°üò¢üò£üò§üò•üò¶üòßüò®üò©üò™üò´üò¨üò≠üòÆüòØüò∞üò±üò≤üò≥üò¥üòµüò∂üò∑üò∏üòπüò∫üòªüòºüòΩüòæüòøüôÄüôÅüôÇüôÉüôÑüôÖüôÜüôáüôàüôâüôäüôãüôåüôçüôéüôèüöÄüöÅüöÇüöÉüöÑüöÖüöÜüöáüöàüöâüöäüöãüöåüöçüöéüöèüöêüöëüöíüöìüöîüöïüöñüöóüöòüöôüööüöõüöúüöùüöûüöüüö†üö°üö¢üö£üö§üö•üö¶üößüö®üö©üö™üö´üö¨üö≠üöÆüöØüö∞üö±üö≤üö≥üö¥üöµüö∂üö∑üö∏üöπüö∫üöªüöºüöΩüöæüöøüõÄüõÅüõÇüõÉüõÑüõÖüõãüõåüõçüõéüõèüõêüõëüõí\U0001f6d5\U0001f6d6\U0001f6d7üõ†üõ°üõ¢üõ£üõ§üõ•üõ©üõ´üõ¨üõ∞üõ≥üõ¥üõµüõ∂\U0001f6f7\U0001f6f8\U0001f6f9\U0001f6fa\U0001f6fb\U0001f6fc\U0001f7e0\U0001f7e1\U0001f7e2\U0001f7e3\U0001f7e4\U0001f7e5\U0001f7e6\U0001f7e7\U0001f7e8\U0001f7e9\U0001f7ea\U0001f7eb\U0001f90c\U0001f90d\U0001f90e\U0001f90fü§êü§ëü§íü§ìü§îü§ïü§ñü§óü§òü§ôü§öü§õü§úü§ùü§û\U0001f91fü§†ü§°ü§¢ü§£ü§§ü§•ü§¶ü§ß\U0001f928\U0001f929\U0001f92a\U0001f92b\U0001f92c\U0001f92d\U0001f92e\U0001f92fü§∞\U0001f931\U0001f932ü§≥ü§¥ü§µü§∂ü§∑ü§∏ü§πü§∫ü§ºü§Ωü§æ\U0001f93fü•Äü•Åü•Çü•Éü•Ñü•Öü•áü•àü•âü•äü•ã\U0001f94c\U0001f94d\U0001f94e\U0001f94fü•êü•ëü•íü•ìü•îü•ïü•ñü•óü•òü•ôü•öü•õü•úü•ùü•û\U0001f95f\U0001f960\U0001f961\U0001f962\U0001f963\U0001f964\U0001f965\U0001f966\U0001f967\U0001f968\U0001f969\U0001f96a\U0001f96b\U0001f96c\U0001f96d\U0001f96e\U0001f96f\U0001f970\U0001f971\U0001f972\U0001f973\U0001f974\U0001f975\U0001f976\U0001f977\U0001f978\U0001f97a\U0001f97b\U0001f97c\U0001f97d\U0001f97e\U0001f97fü¶Äü¶Åü¶Çü¶Éü¶Ñü¶Öü¶Üü¶áü¶àü¶âü¶äü¶ãü¶åü¶çü¶éü¶èü¶êü¶ë\U0001f992\U0001f993\U0001f994\U0001f995\U0001f996\U0001f997\U0001f998\U0001f999\U0001f99a\U0001f99b\U0001f99c\U0001f99d\U0001f99e\U0001f99f\U0001f9a0\U0001f9a1\U0001f9a2\U0001f9a3\U0001f9a4\U0001f9a5\U0001f9a6\U0001f9a7\U0001f9a8\U0001f9a9\U0001f9aa\U0001f9ab\U0001f9ac\U0001f9ad\U0001f9ae\U0001f9af\U0001f9b0\U0001f9b1\U0001f9b2\U0001f9b3\U0001f9b4\U0001f9b5\U0001f9b6\U0001f9b7\U0001f9b8\U0001f9b9\U0001f9ba\U0001f9bb\U0001f9bc\U0001f9bd\U0001f9be\U0001f9bfüßÄ\U0001f9c1\U0001f9c2\U0001f9c3\U0001f9c4\U0001f9c5\U0001f9c6\U0001f9c7\U0001f9c8\U0001f9c9\U0001f9ca\U0001f9cb\U0001f9cd\U0001f9ce\U0001f9cf\U0001f9d0\U0001f9d1\U0001f9d2\U0001f9d3\U0001f9d4\U0001f9d5\U0001f9d6\U0001f9d7\U0001f9d8\U0001f9d9\U0001f9da\U0001f9db\U0001f9dc\U0001f9dd\U0001f9de\U0001f9df\U0001f9e0\U0001f9e1\U0001f9e2\U0001f9e3\U0001f9e4\U0001f9e5\U0001f9e6\U0001f9e7\U0001f9e8\U0001f9e9\U0001f9ea\U0001f9eb\U0001f9ec\U0001f9ed\U0001f9ee\U0001f9ef\U0001f9f0\U0001f9f1\U0001f9f2\U0001f9f3\U0001f9f4\U0001f9f5\U0001f9f6\U0001f9f7\U0001f9f8\U0001f9f9\U0001f9fa\U0001f9fb\U0001f9fc\U0001f9fd\U0001f9fe\U0001f9ff\U0001fa70\U0001fa71\U0001fa72\U0001fa73\U0001fa74\U0001fa78\U0001fa79\U0001fa7a\U0001fa80\U0001fa81\U0001fa82\U0001fa83\U0001fa84\U0001fa85\U0001fa86\U0001fa90\U0001fa91\U0001fa92\U0001fa93\U0001fa94\U0001fa95\U0001fa96\U0001fa97\U0001fa98\U0001fa99\U0001fa9a\U0001fa9b\U0001fa9c\U0001fa9d\U0001fa9e\U0001fa9f\U0001faa0\U0001faa1\U0001faa2\U0001faa3\U0001faa4\U0001faa5\U0001faa6\U0001faa7\U0001faa8\U0001fab0\U0001fab1\U0001fab2\U0001fab3\U0001fab4\U0001fab5\U0001fab6\U0001fac0\U0001fac1\U0001fac2\U0001fad0\U0001fad1\U0001fad2\U0001fad3\U0001fad4\U0001fad5\U0001fad6\U000e0062\U000e0063\U000e0065\U000e0067\U000e006c\U000e006e\U000e0073\U000e0074\U000e0077' 
    
    # Simple Symbols
    
    #'><'
    ',./?!@#_-=+~`"\';:$%*&^()[]{}'
    
    '„ÄÅ„ÄÇ„Ää„Äã„Äå„Äç„Äé„Äè|\\¬∂¬ß‚åò' 
    
    # Fractions
    
    '‚Öü¬Ω‚Öì‚Öï‚Öô‚Öõ‚Öî‚Öñ‚Öö‚Öú¬æ‚Öó‚Öù‚Öû‚Öò¬º‚Öê‚Öë‚Öí‚Üâ%‚ÑÖ‚Ä∞‚Ä±'
    
    # Technicals
    
    '‚åÄ‚åÇ‚åÉ‚åÑ‚åÖ‚åÜ‚åá‚åà‚åâ‚åä‚åã‚åå‚åç‚åé‚åè‚åê‚åë‚åí‚åì‚åî‚åï‚åñ‚åó‚åò‚åô‚åö‚åõ‚åú‚åù‚åû‚åü‚å†‚å°‚å¢‚å£‚å§‚å•‚å¶‚åß‚å®‚å´‚å¨‚å≠‚åÆ‚åØ‚å∞‚å±‚å≤‚å≥‚å¥‚åµ‚å∂‚å∑‚å∏‚åπ‚å∫‚åª‚åº‚åΩ‚åæ‚åø‚çÄ‚çÅ‚çÇ‚çÉ‚çÑ‚çÖ‚çÜ‚çá‚çà‚çâ‚çä‚çã‚çå‚çç‚çé‚çè‚çê‚çë‚çí‚çì‚çî‚çï‚çñ‚çó‚çò‚çô‚çö‚çõ‚çú‚çù‚çû‚çü‚ç†‚ç°‚ç¢‚ç£‚ç§‚ç•‚ç¶‚çß‚ç®‚ç©‚ç™‚ç´‚ç¨‚ç≠‚çÆ‚çØ‚ç∞‚ç±‚ç≤‚ç≥‚ç¥‚çµ‚ç∂‚ç∑‚ç∏‚çπ‚ç∫ÔπòÔπùÔπûÔπüÔπ°„Ä∂‚êõ‚ê°‚êö‚êü‚êò‚ê†‚ê§‚êã‚êå‚êç‚êé‚êè‚êê‚êë‚êí‚êì‚êî‚êï‚êñ‚êó‚êô‚êú‚êù‚êû‚êÄ‚êÅ‚êÇ‚êÉ‚êÑ‚êÖ‚êÜ‚êá‚êà‚êâ‚êä‚ê¢‚ê£‚éãÔ£ø'
    
    # Rectangles
    
    '‚ùè‚ùê‚ùë‚ùí‚ñÄ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñâ‚ñä‚ñã‚ñà‚ñå‚ñê‚ñç‚ñé‚ñè‚ñï‚ñë‚ñí‚ñì‚ñî‚ñ¨‚ñ¢‚ñ£‚ñ§‚ñ•‚ñ¶‚ñß‚ñ®‚ñ©‚ñ™‚ñ´‚ñ≠‚ñÆ‚ñØ‚ò∞‚ò≤‚ò±‚ò¥‚òµ‚ò∂‚ò≥‚ò∑‚ñ∞‚ñ±‚óß‚ó®‚ó©‚ó™‚ó´‚àé‚ñ†‚ñ°‚äû‚äü‚ä†‚ä°‚ùò‚ùô‚ùö„Äì‚óä‚óà‚óá‚óÜ‚éî‚éö‚òñ‚òó'
    
    # Triangles
    
    '‚óÑ‚ñ≤‚ñº‚ñ∫‚óÄ‚ó£‚ó•‚ó§‚ó¢‚ñ∂‚óÇ‚ñ¥‚ñæ‚ñ∏‚óÅ‚ñ≥‚ñΩ‚ñ∑‚àÜ‚àá‚ä≥‚ä≤‚ä¥‚äµ‚óÖ‚ñª‚ñµ‚ñø‚óÉ‚ñπ‚ó≠‚óÆ‚´∑‚´∏‚ãñ‚ãó‚ã™‚ã´‚ã¨‚ã≠‚äø‚ó¨‚âú‚ëÖ'
    
    # Lines 
    
    '‚îÇ‚îÉ‚ïΩ‚ïø‚ïè‚ïë‚ïé‚îáÔ∏±‚îäÔ∏≥‚îã‚îÜ‚ïµ„Ä°„Ä¢‚ïπ‚ïª‚ï∑„Ä£‚ò∞‚ò±‚ò≤‚ò≥‚ò¥‚òµ‚ò∂‚ò∑‚â°‚úï‚ïê‚îÅ‚îÄ‚ïç‚îÖ‚îâ‚îÑ‚îà‚ïå‚ï¥‚ï∂‚ï∏‚ï∫‚ïº‚ïæÔπâÔπçÔπäÔπéÔ∏≤‚ëÜ‚ëá‚ëà‚ëâ‚ëä‚ëÑ‚ëÄÔ∏¥ÔπèÔπåÔπã‚ï≥‚ï≤‚ï±Ô∏∂Ô∏µ„Äµ„Ä¥„Ä≥„ÄÜ`·êü‚Äê‚ÅÉ‚éØ„ÄÑ'
    
    # Corners
    
    'ÔπÑÔπÉÔπÇÔπÅ‚îï‚îì‚îî‚îê‚îñ‚îí‚îó‚îë‚îç‚îô‚îè‚îõ‚îé‚îö‚îå‚îò„Äå„Äç„Äé„ÄèÀ©À•‚îú‚îù‚îû‚îü‚î†‚î°‚î¢‚î£‚î§‚î•‚î¶‚îß‚î®‚î©‚î™‚î´‚î¨‚î≠‚îÆ‚îØ‚î∞‚î±‚î≤‚î≥‚î¥‚îµ‚î∂‚î∑‚î∏‚îπ‚î∫‚îª‚îº‚îΩ‚îæ‚îø‚ïÄ‚ïÅ‚ïÇ‚ïÉ‚ïÑ‚ïÖ‚ïÜ‚ïá‚ïà‚ïâ‚ïä‚ïã‚ïí‚ïï‚ïì‚ïñ‚ïî‚ïó‚ïò‚ïõ‚ïô‚ïú‚ïö‚ïù‚ïû‚ï°‚ïü‚ï¢‚ï†‚ï£‚ï•‚ï®‚ïß‚ï§‚ï¶‚ï©‚ï™‚ï´‚ï¨„Äí‚ä¢‚ä£‚ä§‚ä•‚ï≠‚ïÆ‚ïØ‚ï∞‚ä¶‚äß‚ä®‚ä©‚ä™‚ä´‚ä¨‚ä≠‚äÆ‚äØ‚ä∫„Ä¶„Äß„Ä®À¶ÀßÀ®‚ëÅ‚ëÇ‚ëÉ‚àü'
    
    # Circles
    
    '‚óâ‚óã‚óå‚óç‚óé‚óè‚óê‚óë‚óí‚óì‚óî‚óï‚óñ‚óó‚ùÇ‚ò¢‚äó‚äô‚óò‚óô‚óö‚óõ‚óú‚óù‚óû‚óü‚ó†‚ó°‚óØ„Äá„Ä∂‚ö´‚¨§‚ó¶‚àÖ‚àò‚äï‚äñ‚äò‚äö‚äõ‚äú‚äù‚ùç‚¶ø'
    
    # Comparisons
    
    '‚â§‚â•‚â¶‚âß‚â®‚â©‚ä∞‚ä±‚ãõ‚ãö‚âÇ‚âÉ‚âÑ‚âÖ‚âÜ‚âá‚âà‚ââ‚âä‚âã‚âå‚âç‚âé‚âè‚âê‚âë‚âí‚âì‚âî‚âï‚âñ‚âó‚âò‚âô‚âö‚âõ‚âú‚âù‚âû‚âü‚â†‚â°‚â¢‚â£'
    
    # Numerals
    '‚íà‚íâ‚íä‚íã‚íå‚íç‚íé‚íè‚íê‚íë‚íí‚íì‚íî‚íï‚íñ‚íó‚íò‚íô‚íö‚íõ‚ìø‚ù∂‚ù∑‚ù∏‚ùπ‚ù∫‚ùª‚ùº‚ùΩ‚ùæ‚ùø‚ûÄ‚ûÅ‚ûÇ‚ûÉ‚ûÑ‚ûÖ‚ûÜ‚ûá‚ûà‚ûâ‚ë™‚ë´‚ë¨‚ë≠‚ëÆ‚ëØ‚ë∞‚ë±‚ë≤‚ë≥‚ì™‚ë†‚ë°‚ë¢‚ë£‚ë§‚ë•‚ë¶‚ëß‚ë®‚ë©‚ìµ‚ì∂‚ì∑‚ì∏‚ìπ‚ì∫‚ìª‚ìº‚ìΩ‚ìæ‚ë¥‚ëµ‚ë∂‚ë∑‚ë∏‚ëπ‚ë∫‚ëª‚ëº‚ëΩ‚ëæ‚ëø‚íÄ‚íÅ‚íÇ‚íÉ‚íÑ‚íÖ‚íÜ‚íá‚ûä‚ûã‚ûå‚ûç‚ûé‚ûè‚ûê‚ûë‚ûí‚ûì‚ì´‚ì¨‚ì≠‚ìÆ‚ìØ‚ì∞‚ì±‚ì≤‚ì≥‚ì¥'
    
    '‚Ö†‚Ö°‚Ö¢‚Ö£‚Ö§‚Ö•‚Ö¶‚Öß‚Ö®‚Ö©‚Ö™‚Ö´‚Ö¨‚Ö≠‚ÖÆ‚ÖØ‚Ö∞‚Ö±‚Ö≤‚Ö≥‚Ö¥‚Öµ‚Ö∂‚Ö∑‚Ö∏‚Öπ‚Ö∫‚Öª‚Öº‚ÖΩ‚Öæ‚Öø‚ÜÄ‚ÜÅ‚ÜÇ‚ûÄ‚ûÅ‚ûÇ‚ûÉ‚ûÑ‚ûÖ‚ûÜ‚ûá‚ûà‚ûâ‚ûä‚ûã‚ûå‚ûç‚ûé‚ûè‚ûê‚ûë‚ûí‚ûì‚ìµ‚ì∂‚ì∑‚ì∏‚ìπ‚ì∫‚ìª‚ìº‚ìΩ‚ìæ‚ìø‚ù∂‚ù∑‚ù∏‚ùπ‚ù∫‚ùª‚ùº‚ùΩ‚ùæ‚ùø‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ‚ì™‚ë†‚ë°‚ë¢‚ë£‚ë§‚ë•‚ë¶‚ëß‚ë®‚ë©‚ë™‚ë´‚ë¨‚ë≠‚ëÆ‚ëØ‚ë∞‚ë±‚ë≤‚ë≥‚ë¥‚ëµ‚ë∂‚ë∑‚ë∏‚ëπ‚ë∫‚ëª‚ëº‚ëΩ‚ëæ‚ëø‚íÄ‚íÅ‚íÇ‚íÉ‚íÑ‚íÖ‚íÜ‚íá‚íà‚íâ‚íä‚íã‚íå‚íç‚íé‚íè‚íê‚íë‚íí‚íì‚íî‚íï‚íñ‚íó‚íò‚íô‚íö‚íõ„à†„à°„à¢„à£„à§„à•„à¶„àß„à®„à©„äÄ„äÅ„äÇ„äÉ„äÑ„äÖ„äÜ„äá„äà„äâÔºêÔºëÔºíÔºìÔºîÔºïÔºñÔºóÔºòÔºô‚Å±‚Çê‚Çë‚Çí‚Çì‚Çî'
    
    # Currencies
    
    '‚Ç≥‡∏øÔø†‚Ç°¬¢‚Ç¢‚Çµ‚Ç´‚Ç¨Ôø°¬£‚Ç§‚Ç£∆í‚Ç≤‚Ç≠‚Ç•‚Ç¶‚Ç±ÔºÑ$‚ÇÆ‚Ç©Ôø¶¬•Ôø•‚Ç¥¬§‚Ç∞·üõ‚Ç™‚ÇØ‚Ç†‚Çß‚Ç®‡ØπÔ∑º‡ß≤‡ß≥‚Çπ' 
    
    # Mathematic Symbols
    
    '‚àû‚üÄ‚üÅ‚üÇ‚üÉ‚üÑ‚üá‚üà‚üâ‚üä‚üê‚üë‚üí‚üì‚üî‚üï‚üñ‚üó‚üò‚üô‚üö‚üõ‚üú‚üù‚üû‚üü‚ü†‚ü°‚ü¢‚ü£‚ü§‚ü•‚ü¶‚üß‚ü®‚ü©‚ü™‚ü´‚¶Ä‚¶Å‚¶Ç‚¶É‚¶Ñ‚¶Ö‚¶Ü‚¶á‚¶à‚¶â‚¶ä‚¶ã‚¶å‚¶ç‚¶é‚¶è‚¶ê‚¶ë‚¶í‚¶ì‚¶î‚¶ï‚¶ñ‚¶ó‚¶ò‚¶ô‚¶ö‚¶õ‚¶ú‚¶ù‚¶û‚¶ü‚¶†‚¶°‚¶¢‚¶£‚¶§‚¶•‚¶¶‚¶ß‚¶®‚¶©‚¶™‚¶´‚¶¨‚¶≠‚¶Æ‚¶Ø‚¶∞‚¶±‚¶≤‚¶≥‚¶¥‚¶µ‚¶∂‚¶∑‚¶∏‚¶π‚¶∫‚¶ª‚¶º‚¶Ω‚¶æ‚¶ø‚ßÄ‚ßÅ‚ßÇ‚ßÉ‚ßÑ‚ßÖ‚ßÜ‚ßá‚ßà‚ßâ‚ßä‚ßã‚ßå‚ßç‚ßé‚ßè‚ßê‚ßë‚ßí‚ßì‚ßî‚ßï‚ßñ‚ßó‚ßò‚ßô‚ßö‚ßõ‚ßú‚ßù‚ßû‚ßü‚ß°‚ß¢‚ß£‚ß§‚ß•‚ß¶‚ßß‚ß®‚ß©‚ß™‚ß´‚ß¨‚ß≠‚ßÆ‚ßØ‚ß∞‚ß±‚ß≤‚ß≥‚ß¥‚ßµ‚ß∂‚ß∑‚ß∏‚ßπ‚ß∫‚ßª‚ßº‚ßΩ‚ßæ‚ßø‚àÄ‚àÅ‚àÇ‚àÉ‚àÑ‚àÖ‚àÜ‚àá‚àà‚àâ‚àä‚àã‚àå‚àç‚àé‚àè‚àê‚àë‚àí‚àì‚àî‚àï‚àñ‚àó‚àò‚àô‚àö‚àõ‚àú‚àù‚àü‚à†‚à°‚à¢‚à£‚à§‚à•‚à¶‚àß‚à®‚à©‚à™‚à´‚à¨‚à≠‚àÆ‚àØ‚à∞‚à±‚à≤‚à≥‚à¥‚àµ‚à∂‚à∑‚à∏‚àπ‚à∫‚àª‚àº‚àΩ‚àæ‚àø‚âÄ‚âÅ‚âÇ‚âÉ‚âÑ‚âÖ‚âÜ‚âá‚âà‚ââ‚âä‚âã‚âå‚âç‚âé‚âè‚âê‚âë‚âí‚âì‚âî‚âï‚âñ‚âó‚âò‚âô‚âö‚âõ‚âú‚âù‚âû‚âü‚â†‚â°‚â¢‚â£‚â§‚â•‚â¶‚âß‚â®‚â©‚â™‚â´‚â¨‚â≠‚â∞‚â±‚â≤‚â≥‚â¥‚âµ‚â∂‚â∑‚â∏‚âπ‚â∫‚âª‚âº‚âΩ‚âæ‚âø‚äÄ‚äÅ‚äÇ‚äÉ‚äÑ‚äÖ‚äÜ‚äá‚äà‚äâ‚ää‚äã‚äå‚äç‚äé‚äè‚äê‚äë‚äí‚äì‚äî‚äï‚äñ‚äó‚äò‚äô‚äö‚äõ‚äú‚äù‚äû‚äü‚ä†‚ä°‚ä¢‚ä£‚ä§‚ä•‚ä¶‚äß‚ä®‚ä©‚ä™‚ä´‚ä¨‚ä≠‚äÆ‚äØ‚ä∞‚ä±‚ä≤‚ä≥‚ä¥‚äµ‚ä∂‚ä∑‚ä∏‚äπ‚ä∫‚äª‚äº‚äΩ‚äæ‚äø‚ãÄ‚ãÅ‚ãÇ‚ãÉ‚ãÑ‚ãÖ‚ãÜ‚ãá‚ãà‚ãâ‚ãä‚ãã‚ãå‚ãç‚ãé‚ãè‚ãê‚ãë‚ãí‚ãì‚ãî‚ãï‚ãñ‚ãó‚ãò‚ãô‚ãö‚ãõ‚ãú‚ãù‚ãû‚ãü‚ã†‚ã°‚ã¢‚ã£‚ã§‚ã•‚ã¶‚ãß‚ã®‚ã©‚ã™‚ã´‚ã¨‚ã≠‚ãÆ‚ãØ‚ã∞‚ã±‚ã≤‚ã≥‚ã¥‚ãµ‚ã∂‚ã∑‚ã∏‚ãπ‚ã∫‚ãª‚ãº‚ãΩ‚ãæ‚ãø‚úï‚úñ‚úö√∑¬∞'
    
    # Maths
    
    'œÄ‚àûŒ£‚àö‚àõ‚àú‚à´‚à¨‚à≠‚àÆ‚àØ‚à∞‚à±‚à≤‚à≥‚àÄ‚àÅ‚àÇ‚àÉ‚àÑ‚àÖ‚àÜ‚àá‚àà‚àâ‚àä‚àã‚àå‚àç‚àé‚àè‚àê‚àë‚àí‚àì‚àî‚àï‚àñ‚àó‚àò‚àô‚àù‚àü‚à†‚à°‚à¢‚à£‚à§‚à•‚à¶‚àß‚à®‚à©‚à™‚à¥‚àµ‚à∂‚à∑‚à∏‚àπ‚à∫‚àª‚àº‚àΩ‚àæ‚àø‚âÄ‚âÅ‚âÇ‚âÉ‚âÑ‚âÖ‚âÜ‚âá‚âà‚ââ‚âä‚âã‚âå‚âç‚âé‚âè‚âê‚âë‚âí‚âì‚âî‚âï‚âñ‚âó‚âò‚âô‚âö‚âõ‚âú‚âù‚âû‚âü‚â†‚â°‚â¢‚â£‚â§‚â•‚â¶‚âß‚â®‚â©‚â™‚â´‚â¨‚â≠‚â∞‚â±‚â≤‚â≥‚â¥‚âµ‚â∂‚â∑‚â∏‚âπ‚â∫‚âª‚âº‚âΩ‚âæ‚âø‚äÄ‚äÅ‚äÇ‚äÉ‚äÑ‚äÖ‚äÜ‚äá‚äà‚äâ‚ää‚äã‚äå‚äç‚äé‚äè‚äê‚äë‚äí‚äì‚äî‚äï‚äñ‚äó‚äò‚äô‚äö‚äõ‚äú‚äù‚äû‚äü‚ä†‚ä°‚ä¢‚ä£‚ä§‚ä•‚ä¶‚äß‚ä®‚ä©‚ä™‚ä´‚ä¨‚ä≠‚äÆ‚äØ‚ä∞‚ä±‚ä≤‚ä≥‚ä¥‚äµ‚ä∂‚ä∑‚ä∏‚äπ‚ä∫‚äª‚äº‚äΩ‚äæ‚äø‚ãÄ‚ãÅ‚ãÇ‚ãÉ‚ãÑ‚ãÖ‚ãÜ‚ãá‚ãà‚ãâ‚ãä‚ãã‚ãå‚ãç‚ãé‚ãè‚ãê‚ãë‚ãí‚ãì‚ãî‚ãï‚ãñ‚ãó‚ãò‚ãô‚ãö‚ãõ‚ãú‚ãù‚ãû‚ãü‚ã†‚ã°‚ã¢‚ã£‚ã§‚ã•‚ã¶‚ãß‚ã®‚ã©‚ã™‚ã´‚ã¨‚ã≠‚ãÆ‚ãØ‚ã∞‚ã±‚Å∫‚Åª‚Åº‚ÅΩ‚Åæ‚Åø‚Çä‚Çã‚Çå‚Çç‚Çé‚úñÔπ¢Ôπ£ÔºãÔºçÔºèÔºù√∑¬±√ó',
    
    # Braille Patterns
    '‚†Å‚†Ç‚†Ñ‚†à‚†ê‚††‚†É‚†Ö‚†Ü‚†ò‚†®‚†∞‚†â‚†í‚†§‚†ë‚†°‚†¢‚†ä‚†å‚†î‚†á‚†∏‚†é‚†±‚†£‚†ú‚†™‚†ï‚†ã‚†ô‚†ì‚†ö‚†ç‚†©‚†•‚†¨‚†ñ‚†≤‚†¶‚†¥‚†è‚†π‚†ß‚†º‚†´‚†ù‚†Æ‚†µ‚†∫‚†ó‚†û‚†≥‚†õ‚†≠‚†∂‚†ü‚†ª‚†∑‚†æ‚†Ø‚†Ω‚†ø‚°Ä‚°Ñ‚°Ü‚°á‚°è‚°õ‚°ú‚°ü‚°∂‚°∑‚°º‚°æ‚°ø‚¢Ä‚¢â‚¢†‚¢£‚¢§‚¢ß‚¢∞‚¢∏‚¢π‚¢ª‚¢ø‚£Ä‚£Å‚£Ñ‚£Ü‚£á‚£â‚£í‚£ï‚£ò‚£ô‚£õ‚£†‚£§‚£•‚£¶‚£ß‚£©‚£¨‚£≠‚£∞‚£¥‚£µ‚£∂‚£∑‚£∏‚£π‚£ª‚£º‚£æ‚£ø'
    
    # Zhuyin
    
    '„Ñç„Ñé„Ñ´„Ñê„Ñë„Ñ¨„Ñâ„Ñä„Ñã„ÑÖ„ÑÜ„Ñá„Ñà„Ñ™„Ñó„Ñò„Ñô„Ñì„Ñî„Ñï„Ñè„Ñí„Ñå„Ññ„Ñß„Ñ®„Ñ©„Ñö„Ñõ„Ñù„Ñü„Ñû„Ñ†„Ñ°„Ñ¢„Ñ§„Ñ£„Ñ•„Ñ¶'
    
    # Gender
    
    '‚ôÄ‚ôÇ‚òπ‚ò∫‚òª‚òø„Ä†„ÉÖ„ÉÑ„ã°ÏõÉÏú†√º√úÿ™„Ç∑„ÉÉ„ãõÏõÉÕüÃüÍëáÍê¶Íê†Íê°ÍêïÍåáÍåàÍâïÍàãÍàåÍÜõÍÜúÍÉº‚ò†‚òÉ„Ä≤„Ä¥œ°Ô≠¢‚ç¢‚ç£‚ç§‚ç•‚ç®‚ç©·ΩÉ·Ωï·Ω£—∂”™”´‚ö£‚ö§‚ö•‚ö¶‚öß‚ö®‚ö¢'
    
    # Musical Symbols
    
    '‚ô©‚ô™‚ô´‚ô¨‚ô≠‚ôÆ‚ôØ¬∞√∏ÿÇ‚â†‚â≠'
    
    # Punctuations
    
    '¬∑‚Äë‚Äí‚Äì‚Äî‚Äï‚Äó‚Äò‚Äô‚Äö‚Äõ‚Äú‚Äù‚Äû‚Äü‚Ä¢‚Ä£‚Ä§‚Ä•‚Ä¶‚Äß‚Ä≤‚Ä≥‚Ä¥‚Äµ‚Ä∂‚Ä∑‚ùõ‚ùú‚ùù‚ùû π ∫ ª º Ω æ øÀÄÀÅÀÇÀÉÀÑÀÖÀÜÀáÀàÀâÀäÀãÀåÀçÀéÀèÀêÀëÀíÀìÀîÀïÀñÀóÀòÀôÀöÀõÀúÀùÀûÀ†À°ÔΩû¬øÔπêÔπíÔπîÔπïÔºÅÔºÉÔºÑÔºÖÔºÜÔºäÔºåÔºéÔºöÔºõÔºüÔº†„ÄÅ„ÄÇ„ÄÉ„Äù„ÄûÔ∏∞'
    
    # Ticks / Cross
    
    '‚úì‚úî‚úó‚úò‚òì‚à®‚àö‚úá‚òê‚òë‚òí„Ä§„Ä•',

    # Stars

    '‚òÖ‚òÜ‚âõ‚ãÜ‚çü‚ç£‚òÖ‚òÜ‚ú°‚ú¶‚úß‚ú™‚ú´‚ú¨‚úØ‚ú∞‚ú¥‚úµ‚ú∂‚ú∑‚ú∏',
    
    # Hearts
    
    '‚ô•‚ô°‚ù§‚ù•‚ù£‚ù¶‚ùß·Éì·É¶€µ·Éö‡¶ì·Éö‚ù§Ô∏èÔ∏èüíôüß°üíöüíõüíúüñ§üíóüíìüíîüíüüíïüíñ‚ù£Ô∏èüíòüíùüíû'
    
    # Astrological & Zodiac Sign Symbols
    
    '‚òÆ‚ò∏‚ôà‚ôâ‚ò™‚ôä‚ôã‚ôå‚ôç‚ôé‚ôè‚ôê‚ôë‚ôí‚ôì‚ò§‚ò•‚òß‚ò®‚ò©‚ò´‚ò¨‚ò≠‚òØ‚òΩ‚òæ‚úô‚úö‚úõ‚úú‚úù‚úû‚úü‚Ä†‚äπ‚Ä°‚ôÅ‚ôÜ‚ùñ‚ôÖ‚ú†‚ú°‚ú¢ÂççÂçê„Ä∑‚ò†‚ò¢‚ò£‚ò¶'

    # Flowers

    '‚úΩ‚úæ‚úø‚ùÄ‚ùÅ‚ùÉ‚ùä‚ùã‚ú§‚ú£‚öò‚öúÍï§Íï•‚òò'
    
    # Arrows
         '‚òöüëà‚òõüëâüñùüñúüñõüñö‚òú‚òûüñ¢üëÜüñû‚òùüñ£üëáüñü‚òü‚Üï‚Üñ‚Üó‚Üò‚Üô‚Üö‚Üõ‚Üú‚Üù‚Üû‚Üü‚Ü†‚Ü°‚Ü¢‚Ü£‚Ü§‚Ü•‚Ü¶‚Üß‚Ü®‚Ü©‚Ü™‚Ü´‚Ü¨‚Ü≠‚ÜÆ‚ÜØ‚Ü∞‚Ü±‚Ü≤‚Ü≥‚Ü¥‚Ü∂‚Ü∑‚Ü∏‚Üπ‚Ü∫‚Üª‚Üº‚ÜΩ‚Üæ‚Üø‚áÄ‚áÅ‚áÇ‚áÉ‚áÑ‚áÖ‚áÜ‚áá‚áà‚áâ‚áä‚áã‚áå‚áç‚áé‚áè‚áï‚áñ‚áó‚áò‚áô‚áö‚áõ‚áú‚áù‚áû‚áü‚á†‚á°‚á¢‚á£‚á§‚á•‚á¶‚áß‚á®‚á©‚á™‚åÖ‚åÜ‚å§‚èé‚ñ∂‚òá‚òà‚òä‚òã‚òå‚òç‚ûî‚ûò‚ûô‚ûö‚ûõ‚ûú‚ûù‚ûû‚ûü‚û†‚û°‚û¢‚û£‚û§‚û•‚û¶‚ûß‚û®‚û©‚û™‚û´‚û¨‚û≠‚ûÆ‚ûØ‚û±‚û≤‚û≥‚û¥‚ûµ‚û∂‚û∑‚û∏‚ûπ‚û∫‚ûª‚ûº‚ûΩ‚ûæ‚§¥‚§µ‚Üµ‚Üì‚Üî‚Üê‚Üí‚Üë‚å¶‚å´‚åß‚á∞‚á´‚á¨‚á≠‚á≥‚áÆ‚áØ‚á±‚á≤‚á¥‚áµ‚á∑‚á∏‚áπ‚á∫‚áë‚áì‚áΩ‚áæ‚áø‚¨≥‚üø‚§â‚§à‚áª‚áº‚¨¥‚§Ä‚¨µ‚§Å‚¨π‚§î‚¨∫‚§ï‚¨∂‚§Ö‚¨ª‚§ñ‚¨∑‚§ê‚¨º‚§ó‚¨Ω‚§ò‚§ù‚§û‚§ü‚§†‚§°‚§¢‚§£‚§§‚§•‚§¶‚§™‚§®‚§ß‚§©‚§≠‚§Æ‚§Ø‚§∞‚§±‚§≤‚§´‚§¨‚¨ê‚¨é‚¨ë‚¨è‚§∂‚§∑‚•Ç‚•É‚•Ñ‚≠Ä‚•±‚•∂‚•∏‚≠Ç‚≠à‚≠ä‚•µ‚≠Å‚≠á‚≠â‚•≤‚≠ã‚≠å‚•≥‚•¥‚•Ü‚•Ö‚•π‚•ª‚¨∞‚•à‚¨æ‚•á‚¨≤‚ü¥‚•∑‚≠É‚•∫‚≠Ñ‚•â‚•∞‚¨ø‚§≥‚•ä‚•ã‚•å‚•ç‚•é‚•è‚•ê‚•ë‚•í‚•ì‚•î‚•ï‚•ñ‚•ó‚•ò‚•ô‚•ö‚•õ‚•ú‚•ù‚•û‚•ü‚•†‚•°‚•¢‚•§‚•£‚••‚•¶‚•®‚•ß‚•©‚•Æ‚•Ø‚•™‚•¨‚•´‚•≠‚§å‚§ç‚§é‚§è‚¨∏‚§ë‚¨±‚ü∏‚üπ‚ü∫‚§Ç‚§É‚§Ñ‚§Ü‚§á‚§ä‚§ã‚≠Ö‚≠Ü‚ü∞‚ü±‚áê‚áí‚áî‚á∂‚üµ‚ü∂‚ü∑‚¨Ñ‚¨Ä‚¨Å‚¨Ç‚¨É‚¨Ö‚¨Ü‚¨á‚¨à‚¨â‚¨ä‚¨ã‚¨å‚¨ç‚üª‚üº‚§í‚§ì‚§ô‚§ö‚§õ‚§ú‚•º‚•Ω‚•æ‚•ø‚§º‚§Ω‚§æ‚§ø‚§∏‚§∫‚§π‚§ª‚•Ä‚•Å‚ü≤‚ü≥'
    
    # Weather related symbols
    
    '¬∞‚ÑÉ‚Ñâœü‚òÄ‚òÅ‚òÇ‚òÉ‚òâ‚òº‚òΩ‚òæ‚ôÅ‚ô®‚ùÑ‚ùÖ‚ùÜ‚òá‚òà‚òÑ„éé„éè„éú„éù„éû„é°„èÑ„èé„èë„èí„èï'
    
    # Others
    
    '‚ãÆ‚ã±‚ÜÄ‚ÜÅ‚ÜÇ‚úø‚úΩ‚úª‚ú∞‚ú©‚ú¶‚úß‚ôï‚ôï·êõ‚ôö·¥•·¥ó·óú„ÑúÍí™'

    # Emoji Skin Tone

    'üèªüèºüèΩüèæüèø'

)

SPECIAL_CHARS = ''.join(sorted(set(SPECIAL_CHARS)))

class CanTokenizer(BaseTokenizer):
    """ Uses Bert WordPiece Tokenizer """

    def __init__(
        self,
        vocab_file: Optional[str] = None,
        unk_token: Union[str, AddedToken] = "<unk>",
        sep_token: Union[str, AddedToken] = "</s>",
        cls_token: Union[str, AddedToken] = "<s>",
        nl_token: Union[str, AddedToken] = "<nl>",
        pad_token: Union[str, AddedToken] = "<pad>",
        mask_token: Union[str, AddedToken] = "<mask>",
        clean_text: bool = True,
        handle_chinese_chars: bool = True,
        separate_numbers: bool = True,
        strip_accents: bool = True,
        lowercase: bool = True,
        wordpieces_prefix: str = "##",
        special_chars: str = SPECIAL_CHARS,
        zh_norm: bool = True,
    ):

        if vocab_file is not None:
            tokenizer = Tokenizer(WordPiece(vocab_file, unk_token=str(unk_token)))
        else:
            tokenizer = Tokenizer(WordPiece())

        # Let the tokenizer know about special tokens if they are part of the vocab
        if tokenizer.token_to_id(str(unk_token)) is not None:
            tokenizer.add_special_tokens([str(unk_token)])
        if tokenizer.token_to_id(str(sep_token)) is not None:
            tokenizer.add_special_tokens([str(sep_token)])
        if tokenizer.token_to_id(str(cls_token)) is not None:
            tokenizer.add_special_tokens([str(cls_token)])
        if tokenizer.token_to_id(str(pad_token)) is not None:
            tokenizer.add_special_tokens([str(pad_token)])
        if tokenizer.token_to_id(str(nl_token)) is not None:
            tokenizer.add_special_tokens([str(nl_token)])
        if tokenizer.token_to_id(str(mask_token)) is not None:
            tokenizer.add_special_tokens([str(mask_token)])
        if tokenizer.token_to_id(str(mask_token)) is not None:
            tokenizer.add_special_tokens([str(mask_token)])

        tokenizer.normalizer = Sequence([NFKC(), BertNormalizer(
            clean_text=clean_text,
            handle_chinese_chars=handle_chinese_chars,
            separate_numbers=separate_numbers,
            strip_accents=strip_accents,
            lowercase=lowercase,
            special_chars=special_chars,
            zh_norm=zh_norm
        )])
        tokenizer.pre_tokenizer = BertPreTokenizer()

        
        tokenizer.decoder = decoders.WordPiece(prefix=wordpieces_prefix)

        parameters = {
            "model": "BertWordPiece",
            "unk_token": unk_token,
            "sep_token": sep_token,
            "cls_token": cls_token,
            "nl_token": nl_token,
            "pad_token": pad_token,
            "mask_token": mask_token,
            "clean_text": clean_text,
            "handle_chinese_chars": handle_chinese_chars,
            "separate_numbers": separate_numbers,
            "strip_accents": strip_accents,
            "lowercase": lowercase,
            "special_chars": special_chars,
            "zh_norm": zh_norm,
            "wordpieces_prefix": wordpieces_prefix,
        }

        super().__init__(tokenizer, parameters)

    def train(
        self,
        files: Union[str, List[str]],
        vocab_size: int = 30000,
        min_frequency: int = 20,
        limit_alphabet: int = 6000,
        initial_alphabet: List[str] = [],
        special_tokens: List[Union[str, AddedToken]] = [
            "<pad>",
            "<unk>",
            "<s>",
            "<nl>",
            "</s>",
            "<mask>",
        ],
        show_progress: bool = True,
        wordpieces_prefix: str = "##",
    ):
        """ Train the model using the given files """

        trainer = trainers.WordPieceTrainer(
            vocab_size=vocab_size,
            min_frequency=min_frequency,
            limit_alphabet=limit_alphabet,
            initial_alphabet=initial_alphabet,
            special_tokens=special_tokens,
            show_progress=show_progress,
            continuing_subword_prefix=wordpieces_prefix,
        )
        if isinstance(files, str):
            files = [files]
        self._tokenizer.train(trainer, files)

        


class CanTokenizerSP(BaseTokenizer):
    """ SentencePiece BPE Tokenizer
    Represents the BPE algorithm, with the pretokenization used by SentencePiece
    """

    def __init__(
        self,
        vocab_file: Optional[str] = None,
        merges_file: Optional[str] = None,
        unk_token: Union[str, AddedToken] = "<unk>",
        replacement: str = "‚ñÅ",
        add_prefix_space: bool = True,
        no_consecutive_space: bool = True,
        dropout: Optional[float] = None,
        clean_text: bool = True,
        handle_chinese_chars: bool = True,
        separate_numbers: bool = True,
        strip_accents: bool = True,
        lowercase: bool = True,
        wordpieces_prefix: str = "##",
        special_chars: str = SPECIAL_CHARS,
        zh_norm: bool = True,
    ):
        if vocab_file is not None and merges_file is not None:
            tokenizer = Tokenizer(
                BPE(vocab_file, merges_file, dropout=dropout, unk_token=unk_token)
            )
        else:
            tokenizer = Tokenizer(BPE())

        if tokenizer.token_to_id(str(unk_token)) is not None:
            tokenizer.add_special_tokens([str(unk_token)])

        tokenizer.normalizer = Sequence([NFKC(), BertNormalizer(
            clean_text=clean_text,
            handle_chinese_chars=handle_chinese_chars,
            separate_numbers=separate_numbers,
            strip_accents=strip_accents,
            lowercase=lowercase,
            special_chars=special_chars,
            zh_norm=zh_norm
        )])
        tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(
            replacement=replacement, add_prefix_space=add_prefix_space, no_consecutive_space=no_consecutive_space
        )
        tokenizer.decoder = decoders.Metaspace(
            replacement=replacement, add_prefix_space=add_prefix_space, no_consecutive_space=no_consecutive_space
        )

        parameters = {
            "model": "SentencePieceBPE",
            "unk_token": unk_token,
            "replacement": replacement,
            "add_prefix_space": add_prefix_space,
            "no_consecutive_space": no_consecutive_space,
            "dropout": dropout,
        }

        super().__init__(tokenizer, parameters)

    def train(
        self,
        files: Union[str, List[str]],
        vocab_size: int = 30000,
        min_frequency: int = 20,
        special_tokens: List[Union[str, AddedToken]] = [
            "<pad>",
            "<unk>",
            "<s>",
            "<nl>",
            "</s>",
            "<mask>",
        ],
        limit_alphabet: int = 6000,
        initial_alphabet: List[str] = [],
        show_progress: bool = True,
    ):
        """ Train the model using the given files """

        trainer = trainers.BpeTrainer(
            vocab_size=vocab_size,
            min_frequency=min_frequency,
            special_tokens=special_tokens,
            limit_alphabet=limit_alphabet,
            initial_alphabet=initial_alphabet,
            show_progress=show_progress,
        )
        if isinstance(files, str):
            files = [files]
        self._tokenizer.train(trainer, files)